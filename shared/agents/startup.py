#!/usr/bin/env python3
"""
Non-interactive launcher for LLM orchestration.

Reads config.json (generated by setup.py), verifies hardware matches
expectations, launches brain + workers. Adapts gracefully if hardware
has changed (degraded mode).

Usage:
  python startup.py                    # Normal start
  python startup.py --brain-only       # Brain only, no workers
  python startup.py --config alt.json  # Use alternate config
"""

import argparse
import json
import os
import signal
import socket
import subprocess
import sys
import time
import urllib.error
import urllib.request
import uuid
from datetime import datetime
from pathlib import Path

from hardware import scan_gpus, scan_ollama, scan_cpu_temps

# =============================================================================
# Tunable wait times (seconds)
# =============================================================================
BRAIN_MAX_WAIT = 300
GPU_MAX_WAIT = 60
FLAG_CHECK_INTERVAL = 0.5

READY_FLAG_DIR = Path("/tmp/llm-orchestration-flags")
LAUNCH_LOCK = READY_FLAG_DIR / "launch.lock"
processes = []


# =============================================================================
# Singleton lock (from launch.py)
# =============================================================================
def check_existing_launch():
    """Check if another launcher is already running. Exit if so."""
    if LAUNCH_LOCK.exists():
        try:
            pid = int(LAUNCH_LOCK.read_text().strip())
            os.kill(pid, 0)
            print(f"ERROR: Another instance is already running (PID {pid})")
            print(f"Kill it first: kill {pid}")
            print(f"Or remove stale lock: rm {LAUNCH_LOCK}")
            sys.exit(1)
        except (ProcessLookupError, ValueError):
            print(
                f"Removing stale lock file "
                f"(old PID: {LAUNCH_LOCK.read_text().strip()})"
            )
            LAUNCH_LOCK.unlink()

    READY_FLAG_DIR.mkdir(parents=True, exist_ok=True)
    LAUNCH_LOCK.write_text(str(os.getpid()))


def remove_launch_lock():
    """Remove the launch lock file."""
    try:
        if LAUNCH_LOCK.exists():
            LAUNCH_LOCK.unlink()
    except Exception:
        pass


# =============================================================================
# Ready flag system (from launch.py)
# =============================================================================
def clear_ready_flags():
    """Remove all ready flag files."""
    if READY_FLAG_DIR.exists():
        for f in READY_FLAG_DIR.glob("*.ready"):
            f.unlink()
    else:
        READY_FLAG_DIR.mkdir(parents=True, exist_ok=True)


def wait_for_ready_flag(name: str, max_wait: int) -> bool:
    """Wait for an agent to signal ready via flag file."""
    flag_file = READY_FLAG_DIR / f"{name}.ready"
    start_time = time.time()
    last_report = 0

    while time.time() - start_time < max_wait:
        if flag_file.exists():
            elapsed = time.time() - start_time
            print(f"  {name} ready ({elapsed:.1f}s)")
            return True

        elapsed = int(time.time() - start_time)
        if elapsed - last_report >= 10:
            print(f"  {name} loading... ({elapsed}s / {max_wait}s)")
            last_report = elapsed

        time.sleep(FLAG_CHECK_INTERVAL)

    print(f"  WARNING: {name} did not signal ready within {max_wait}s")
    return False


def _is_port_open(port: int, host: str = "127.0.0.1") -> bool:
    """Return True if a TCP port is currently accepting connections."""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        sock.settimeout(0.5)
        return sock.connect_ex((host, port)) == 0


def _get_max_cpu_temp_c() -> int | None:
    """Return max live CPU temperature in C, or None when unavailable."""
    try:
        temps = scan_cpu_temps()
        if not temps:
            return None
        return max(t["temp_c"] for t in temps)
    except Exception:
        return None


def _wait_for_cpu_cooldown(config: dict, max_wait_seconds: int = 600) -> bool:
    """Block startup until CPU temp is below warning threshold.

    Returns True when startup can continue, False if cooldown timed out.
    """
    cpu_warn = config.get("resource_limits", {}).get("cpu_temp_warning_c", 80)
    interval = 5
    waited = 0

    while waited <= max_wait_seconds:
        cpu_temp = _get_max_cpu_temp_c()
        if cpu_temp is None:
            print("CPU temp preflight: unavailable (continuing)")
            return True
        if cpu_temp < cpu_warn:
            print(f"CPU temp preflight: {cpu_temp}C (< {cpu_warn}C) OK")
            return True

        print(
            f"CPU temp preflight: {cpu_temp}C >= {cpu_warn}C "
            f"(cooldown {waited}s/{max_wait_seconds}s)"
        )
        time.sleep(interval)
        waited += interval

    return False


def _clear_stale_heartbeats(shared_path: Path, worker_gpus: list):
    """Remove stale GPU/task heartbeat files so startup reflects live state."""
    removed = 0
    for gpu_cfg in worker_gpus:
        hb = shared_path / "gpus" / f"gpu_{gpu_cfg['id']}" / "heartbeat.json"
        if hb.exists():
            try:
                hb.unlink()
                removed += 1
            except OSError:
                pass

    processing_dir = shared_path / "tasks" / "processing"
    if processing_dir.exists():
        for hb in processing_dir.glob("*.heartbeat.json"):
            try:
                hb.unlink()
                removed += 1
            except OSError:
                pass

    if removed:
        print(f"Cleared {removed} stale heartbeat file(s) before startup")


def _reclaim_worker_port(port: int, gpu_name: str):
    """Fail-fast reclaim for worker ports.

    Startup assumes worker ports must be owned by this orchestrator. If a port is
    occupied, attempt to terminate the listener and re-check. If reclaim fails,
    abort startup with a clear error.
    """
    if not _is_port_open(port):
        return

    print(
        f"  WARNING: worker port {port} ({gpu_name}) is in use. "
        f"Attempting reclaim..."
    )

    # Try lsof first (exact PID targeting), then fuser as fallback.
    killed_any = False
    try:
        result = subprocess.run(
            ["lsof", "-t", f"-iTCP:{port}", "-sTCP:LISTEN"],
            capture_output=True,
            text=True,
            check=False,
        )
        pids = [p.strip() for p in result.stdout.splitlines() if p.strip()]
        for pid_str in pids:
            try:
                os.kill(int(pid_str), signal.SIGTERM)
                killed_any = True
            except (ProcessLookupError, PermissionError, ValueError):
                pass
    except FileNotFoundError:
        pass

    if not killed_any:
        subprocess.run(
            ["fuser", "-k", f"{port}/tcp"],
            capture_output=True,
            check=False,
        )

    # Wait briefly for socket close
    deadline = time.time() + 5
    while time.time() < deadline:
        if not _is_port_open(port):
            print(f"  Reclaimed port {port} for {gpu_name}")
            return
        time.sleep(0.2)

    print(
        f"ERROR: Failed to reclaim worker port {port} ({gpu_name}). "
        f"Stop the conflicting process and retry."
    )
    remove_launch_lock()
    sys.exit(1)


def _count_existing_meta_tasks(shared_path: Path, command: str) -> int:
    """Count existing meta tasks with a given command in queue/processing."""
    count = 0
    for folder in ("queue", "processing"):
        path = shared_path / "tasks" / folder
        if not path.exists():
            continue
        for task_file in path.glob("*.json"):
            if str(task_file).endswith(".lock"):
                continue
            try:
                with open(task_file) as f:
                    task = json.load(f)
                if (
                    task.get("task_class") == "meta"
                    and task.get("command") == command
                ):
                    count += 1
            except Exception:
                continue
    return count


def _enqueue_startup_load_llm(shared_path: Path, created_by: str, count: int):
    """Insert N startup load_llm meta tasks into the public queue."""
    if count <= 0:
        return

    queue_path = shared_path / "tasks" / "queue"
    queue_path.mkdir(parents=True, exist_ok=True)

    existing = _count_existing_meta_tasks(shared_path, "load_llm")
    to_add = max(0, count - existing)
    if to_add == 0:
        print(
            f"Startup warm target already satisfied ({existing} load_llm pending)."
        )
        return

    for idx in range(to_add):
        task = {
            "task_id": str(uuid.uuid4()),
            "type": "meta",
            "command": "load_llm",
            "batch_id": "system",
            "name": f"load_llm_startup_{idx + 1}",
            "priority": 10,
            "task_class": "meta",
            "depends_on": [],
            "executor": "worker",
            "status": "pending",
            "created_at": datetime.now().isoformat(),
            "created_by": created_by,
            "retry_count": 0,
        }
        task_file = queue_path / f"{task['task_id']}.json"
        with open(task_file, "w") as f:
            json.dump(task, f, indent=2)

    print(
        f"Queued {to_add} startup load_llm task(s) "
        f"(target={count}, existing={existing})."
    )


# =============================================================================
# Pre-loaded model detection (from launch.py)
# =============================================================================
def check_loaded_models(ollama_host: str) -> dict:
    """Check what models are currently loaded in Ollama."""
    try:
        req = urllib.request.Request(f"{ollama_host}/api/ps")
        with urllib.request.urlopen(req, timeout=5) as resp:
            data = json.loads(resp.read().decode())
            loaded = {}
            for model in data.get("models", []):
                name = model.get("name", "")
                loaded[name] = {
                    "size_vram": model.get("size_vram", 0),
                }
            return loaded
    except (urllib.error.URLError, TimeoutError, json.JSONDecodeError, OSError):
        return {}


# =============================================================================
# Hardware verification
# =============================================================================
def verify_hardware(config: dict, actual_gpus: list, ollama: dict) -> dict:
    """Compare config expectations against actual hardware.

    Returns:
        {
            ok: bool,
            warnings: [str],
            available_workers: [gpu_config],
            missing_workers: [gpu_config],
            brain_ok: bool,
        }
    """
    result = {
        "ok": True,
        "warnings": [],
        "available_workers": [],
        "missing_workers": [],
        "brain_ok": True,
    }

    actual_indices = {g["index"] for g in actual_gpus}
    actual_by_index = {g["index"]: g for g in actual_gpus}

    # Verify brain GPUs
    for brain_gpu_idx in config["brain"]["gpus"]:
        if brain_gpu_idx in actual_indices:
            actual = actual_by_index[brain_gpu_idx]
            print(
                f"  GPU {brain_gpu_idx}: {actual['name']} "
                f"[{actual['vram_mb']} MB] OK"
            )
        else:
            print(
                f"  GPU {brain_gpu_idx}: MISSING — configured as brain"
            )
            result["brain_ok"] = False
            result["ok"] = False
            result["warnings"].append(
                f"Brain GPU {brain_gpu_idx} not found"
            )

    # Verify worker GPUs
    expected_workers = config.get("gpus", [])
    for gpu_config in expected_workers:
        gpu_id = gpu_config["id"]
        if gpu_id in actual_indices:
            actual = actual_by_index[gpu_id]
            print(
                f"  GPU {gpu_id}: {actual['name']} "
                f"[{actual['vram_mb']} MB] OK"
            )
            result["available_workers"].append(gpu_config)
        else:
            print(
                f"  GPU {gpu_id}: MISSING — configured as worker "
                f"{gpu_config['name']}"
            )
            result["missing_workers"].append(gpu_config)
            result["warnings"].append(
                f"Worker GPU {gpu_id} ({gpu_config['name']}) not found"
            )

    if result["missing_workers"]:
        result["ok"] = False
        expected = len(expected_workers)
        available = len(result["available_workers"])
        print(
            f"  WARNING: Expected {expected} worker GPUs, "
            f"only {available} available"
        )

    # Verify Ollama
    if ollama["running"]:
        print("  Ollama: running OK")
    else:
        print("  Ollama: NOT running")
        result["warnings"].append("Ollama is not running")
        result["ok"] = False

    return result


# =============================================================================
# Cleanup (from launch.py)
# =============================================================================
def cleanup(signum=None, frame=None):
    """Clean up all processes."""
    print("\nShutting down agents...")
    remove_launch_lock()

    for name, proc in processes:
        if proc is None:
            continue
        print(f"  Stopping {name}...")
        proc.terminate()

    for name, proc in processes:
        if proc is None:
            continue
        try:
            proc.wait(timeout=10)
        except subprocess.TimeoutExpired:
            proc.kill()

    clear_ready_flags()
    print("All agents stopped.")
    sys.exit(0)


# =============================================================================
# Main
# =============================================================================
def main():
    parser = argparse.ArgumentParser(
        description="Non-interactive launcher for LLM orchestration"
    )
    default_config = Path(__file__).parent / "config.json"
    parser.add_argument("--config", default=str(default_config))
    parser.add_argument(
        "--brain-only", action="store_true", help="Start only brain"
    )
    parser.add_argument(
        "--gpus",
        type=int,
        default=None,
        metavar="N",
        help="Number of GPU agents to start (default: all)",
    )
    parser.add_argument(
        "--initial-hot-workers",
        type=int,
        default=None,
        metavar="N",
        help="Override config initial_hot_workers for this startup run",
    )
    args = parser.parse_args()

    # Load config
    config_path = Path(args.config)
    if not config_path.exists():
        print(f"ERROR: Config file not found: {config_path}")
        print(f"  Run 'python setup.py' to generate one.")
        sys.exit(1)

    print(f"Loading {config_path}...")
    with open(config_path) as f:
        config = json.load(f)

    check_existing_launch()

    # Resolve relative paths
    agents_dir = Path(__file__).parent
    if not Path(config["shared_path"]).is_absolute():
        config["shared_path"] = str(
            (agents_dir / config["shared_path"]).resolve()
        )
    if not Path(config["permissions_path"]).is_absolute():
        config["permissions_path"] = str(
            (agents_dir / config["permissions_path"]).resolve()
        )

    signal.signal(signal.SIGINT, cleanup)
    signal.signal(signal.SIGTERM, cleanup)

    # Verify hardware
    print("\nVerifying hardware and thermal state...")
    ollama_host = config.get("ollama_host", "http://localhost:11434")
    actual_gpus = scan_gpus()
    ollama = scan_ollama(ollama_host)
    cpu_temp = _get_max_cpu_temp_c()
    gpu_temp_max = max((g.get("temp_c", 0) for g in actual_gpus), default=None)

    expected_gpu_count = len(config["brain"]["gpus"]) + len(
        config.get("gpus", [])
    )
    print(f"  Expected {expected_gpu_count} GPUs — found {len(actual_gpus)}")
    if cpu_temp is not None:
        print(f"  CPU temp (live): {cpu_temp}C")
    if gpu_temp_max is not None:
        print(f"  Max GPU temp (live): {gpu_temp_max}C")

    hw = verify_hardware(config, actual_gpus, ollama)

    if not hw["brain_ok"]:
        print("\nERROR: Brain GPU(s) missing. Cannot start.")
        print("  Re-run 'python setup.py' to reconfigure.")
        remove_launch_lock()
        sys.exit(1)

    # Startup thermal gate: require live temps to be below warning threshold
    # before loading/reusing models or launching agents.
    cooldown_ok = _wait_for_cpu_cooldown(config, max_wait_seconds=600)
    if not cooldown_ok:
        cpu_warn = config.get("resource_limits", {}).get("cpu_temp_warning_c", 80)
        cpu_now = _get_max_cpu_temp_c()
        print(
            f"\nERROR: CPU temperature preflight did not cool below "
            f"{cpu_warn}C within 600s (current: {cpu_now}C)."
        )
        print("  Fix cooling/airflow before restart, then retry startup.")
        remove_launch_lock()
        sys.exit(1)

    # Fresh startup status baseline: clear stale heartbeat snapshots from old runs.
    _clear_stale_heartbeats(Path(config["shared_path"]), hw["available_workers"])

    clear_ready_flags()

    # Check if brain model is pre-loaded
    loaded_models = check_loaded_models(ollama_host)
    brain_model = config["brain"]["model"]
    brain_preloaded = brain_model in loaded_models if brain_model else False

    if loaded_models:
        print(f"\nPre-loaded models: {', '.join(loaded_models.keys())}")
        if brain_preloaded:
            print(f"  Brain model ({brain_model}) is pre-loaded")
        skip_ollama_restart = brain_preloaded
    else:
        skip_ollama_restart = False

    if skip_ollama_restart:
        print("\nReusing existing Ollama instance with pre-loaded models")
    else:
        print("\nNo pre-loaded brain model detected; continuing with live Ollama")

    # --- Start Brain ---
    print(f"\nStarting brain on GPUs {config['brain']['gpus']}...")
    brain_cmd = [
        sys.executable,
        str(agents_dir / "brain.py"),
        "--config",
        args.config,
    ]
    if brain_preloaded:
        brain_cmd.append("--model-preloaded")

    brain_proc = subprocess.Popen(
        brain_cmd, stdout=sys.stdout, stderr=sys.stderr
    )
    processes.append(("brain", brain_proc))

    wait_time = 60 if brain_preloaded else BRAIN_MAX_WAIT
    if not wait_for_ready_flag("brain", wait_time):
        print("Brain failed to start, aborting...")
        cleanup()

    # --- Start GPU Agents ---
    if args.brain_only:
        gpus_to_start = []
    else:
        gpus_to_start = hw["available_workers"]
        if args.gpus is not None:
            gpus_to_start = gpus_to_start[: args.gpus]

    worker_mode = config.get("worker_mode", "hot")

    if gpus_to_start:
        print(
            f"\nStarting {len(gpus_to_start)} workers "
            f"(mode: {worker_mode})..."
        )

        for gpu_config in gpus_to_start:
            gpu_name = gpu_config["name"]
            gpu_port = gpu_config.get("port")
            print(f"\n  Starting {gpu_name} (GPU {gpu_config['id']})...")

            if gpu_port:
                _reclaim_worker_port(gpu_port, gpu_name)

            gpu_cmd = [
                sys.executable,
                str(agents_dir / "gpu.py"),
                gpu_name,
                "--config",
                args.config,
            ]

            gpu_proc = subprocess.Popen(
                gpu_cmd, stdout=sys.stdout, stderr=sys.stderr
            )
            processes.append((gpu_name, gpu_proc))

            if not wait_for_ready_flag(gpu_name, GPU_MAX_WAIT):
                print(
                    f"WARNING: {gpu_name} may not be ready, "
                    f"continuing anyway..."
                )

    # Optional startup warm-up: queue load_llm meta tasks.
    # Workers still start cold; these tasks selectively heat up N workers.
    if args.initial_hot_workers is not None:
        initial_hot_workers = max(0, int(args.initial_hot_workers))
    else:
        initial_hot_workers = int(config.get("initial_hot_workers", 0))
    if initial_hot_workers > 0 and gpus_to_start:
        warm_target = min(initial_hot_workers, len(gpus_to_start))
        print(f"\nQueueing startup warm-up: {warm_target} worker(s)")
        _enqueue_startup_load_llm(
            shared_path=Path(config["shared_path"]),
            created_by="startup",
            count=warm_target,
        )

    # --- Running ---
    total_workers = len(config.get("gpus", []))
    active_workers = len(gpus_to_start)
    degraded = active_workers < total_workers and not args.brain_only

    print("\n" + "=" * 60)
    if degraded:
        print(
            f"System running (degraded: "
            f"{active_workers}/{total_workers} workers)"
        )
    else:
        print("System running!")
    print("=" * 60)
    print(f"Brain: {config['brain']['model']} on GPUs {config['brain']['gpus']}")

    if gpus_to_start:
        print(f"\nWorkers ({len(gpus_to_start)}, mode: {worker_mode}):")
        for g in gpus_to_start:
            print(
                f"  {g['name']}: GPU {g['id']} - {g['model']} "
                f"(port {g['port']})"
            )
    elif not args.brain_only:
        print("\nWorkers: none started")

    if hw["warnings"]:
        print(f"\nWarnings:")
        for w in hw["warnings"]:
            print(f"  - {w}")

    print("\nPress Ctrl+C to stop all agents")
    print("=" * 60 + "\n")

    # Monitor processes
    while True:
        for i, (name, proc) in enumerate(processes):
            if proc is None:
                continue
            if proc.poll() is not None:
                exit_code = proc.returncode
                print(f"\n{name} exited with code {exit_code}")

                if name == "brain":
                    print("Brain died - shutting down system")
                    cleanup()
                else:
                    print(
                        f"GPU agent {name} died. "
                        f"Tasks assigned to it may need re-queuing."
                    )
                    processes[i] = (name, None)

        alive_gpus = [
            p for name, p in processes if p is not None and name != "brain"
        ]
        if not alive_gpus and gpus_to_start and not args.brain_only:
            print("\nAll GPU agents have died. Brain still running.")
            print("Restart GPU agents manually or press Ctrl+C to stop.")

        time.sleep(5)


if __name__ == "__main__":
    main()
